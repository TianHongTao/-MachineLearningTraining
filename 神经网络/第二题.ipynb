{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二题：神经网络：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实验内容：\n",
    "1. 学会梯度下降的基本思想\n",
    "2. 学会使用梯度下降求解线性回归\n",
    "3. 了解归一化处理的作用"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 线性回归\n",
    "\n",
    "<img src=\"https://davidham3.github.io/blog/2018/09/11/logistic-regression/Fig0.png\" width=300>\n",
    "\n",
    "我们来完成最简单的线性回归，上图是一个最简单的神经网络，一个输入层，一个输出层，没有激活函数。  \n",
    "我们记输入为$X \\in \\mathbb{R}^{n \\times m}$，输出为$Z \\in \\mathbb{R}^{n}$。输入包含了$n$个样本，$m$个特征，输出是对这$n$个样本的预测值。  \n",
    "输入层到输出层的权重和偏置，我们记为$W \\in \\mathbb{R}^{m}$和$b \\in \\mathbb{R}$。  \n",
    "输出层没有激活函数，所以上面的神经网络的前向传播过程写为：\n",
    "\n",
    "$$\n",
    "Z = XW + b\n",
    "$$\n",
    "\n",
    "我们使用均方误差作为模型的损失函数\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, \\hat{y}) = \\frac{1}{n} \\sum^n_{i=1}(y_i - \\hat{y_i})^2\n",
    "$$\n",
    "\n",
    "我们通过调整参数$W$和$b$来降低均方误差，或者说是以降低均方误差为目标，学习参数$W$和参数$b$。当均方误差下降的时候，我们认为当前的模型的预测值$Z$与真值$y$越来越接近，也就是说模型正在学习如何让自己的预测值变得更准确。\n",
    "\n",
    "在前面的课程中，我们已经学习了这种线性回归模型可以使用最小二乘法求解，最小二乘法在求解数据量较小的问题的时候很有效，但是最小二乘法的时间复杂度很高，一旦数据量变大，效率很低，实际应用中我们会使用梯度下降等基于梯度的优化算法来求解参数$W$和参数$b$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度下降\n",
    "\n",
    "梯度下降是一种常用的优化算法，通俗来说就是计算出参数的梯度（损失函数对参数的偏导数的导数值），然后将参数减去参数的梯度乘以一个很小的数（下面的公式），来改变参数，然后重新计算损失函数，再次计算梯度，再次进行调整，通过一定次数的迭代，参数就会收敛到最优点附近。\n",
    "\n",
    "在我们的这个线性回归问题中，我们的参数是$W$和$b$，使用以下的策略更新参数：\n",
    "\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$\n",
    "\n",
    "其中，$\\alpha$ 是学习率，一般设置为0.1，0.01等。\n",
    "\n",
    "接下来我们会求解损失函数对参数的偏导数。\n",
    "\n",
    "损失函数MSE记为：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$\n",
    "\n",
    "其中，$Z \\in \\mathbb{R}^{n}$是我们的预测值，也就是神经网络输出层的输出值。这里我们有$n$个样本，实际上是将$n$个样本的预测值与他们的真值相减，取平方后加和。\n",
    "\n",
    "我们计算损失函数对参数$W$的偏导数，根据链式法则，可以将偏导数拆成两项，分别求解后相乘：\n",
    "\n",
    "**这里我们以矩阵的形式写出推导过程，感兴趣的同学可以尝试使用单个样本进行推到，然后推广到矩阵形式**\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial W}\\\\\n",
    "&= - \\frac{2}{n} X^\\mathrm{T} (y - Z)\\\\\n",
    "&= \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "\\end{aligned}$$\n",
    "\n",
    "同理，求解损失函数对参数$b$的偏导数:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} &= \\frac{\\partial \\mathrm{loss}}{\\partial Z} \\frac{\\partial Z}{\\partial b}\\\\\n",
    "&= - \\frac{2}{n} \\sum^n_{i=1}(y_i - Z_i)\\\\\n",
    "&= \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**因为参数$b$对每个样本的损失值都有贡献，所以我们需要将所有样本的偏导数都加和。**\n",
    "\n",
    "其中，$\\frac{\\partial \\mathrm{loss}}{\\partial W} \\in \\mathbb{R}^{m}$，$\\frac{\\partial \\mathrm{loss}}{\\partial b} \\in \\mathbb{R}$，求解得到的梯度的维度与参数一致。\n",
    "\n",
    "完成上式两个梯度的计算后，就可以使用梯度下降法对参数进行更新了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练神经网络的基本思路：\n",
    "\n",
    "1. 首先对参数进行初始化，对参数进行随机初始化（也就是取随机值）\n",
    "2. 将样本输入神经网络，计算神经网络预测值 $Z$\n",
    "3. 计算损失值MSE\n",
    "4. 通过 $Z$ 和 $y$ ，以及 $X$ ，计算参数的梯度\n",
    "5. 使用梯度下降更新参数\n",
    "6. 循环1-5步，**在反复迭代的过程中可以看到损失值不断减小的现象，如果没有下降说明出了问题**\n",
    "\n",
    "接下来我们来实现这个最简单的神经网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 导入数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用kaggle房价数据，选3列作为特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# 读取数据\n",
    "data = pd.read_csv('data/kaggle_house_price_prediction/kaggle_hourse_price_train.csv')\n",
    "\n",
    "# 使用这3列作为特征\n",
    "features = ['LotArea', 'BsmtUnfSF', 'GarageArea']\n",
    "target = 'SalePrice'\n",
    "data = data[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "40%做测试集，60%做训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "trainX, testX, trainY, testY = train_test_split(data[features], data[target], test_size = 0.4, random_state = 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集876个样本，3个特征，测试集584个样本，3个特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((876, 3), (876,), (584, 3), (584,))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape, trainY.shape, testX.shape, testY.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 参数初始化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要初始化参数$W$和$b$，其中$W \\in \\mathbb{R}^m$，$b \\in \\mathbb{R}$，初始化的策略是将$W$初始化成一个随机数矩阵，参数$b$为0。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(m):\n",
    "    '''\n",
    "    参数初始化，将W初始化成一个随机向量，b是一个长度为1的向量\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m: int, 特征数\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    W: np.ndarray, shape = (m, ), 参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, ), 参数b\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 指定随机种子，这样生成的随机数就是固定的了，这样就可以与下面的测试样例进行比对\n",
    "    np.random.seed(32)\n",
    "    \n",
    "    W = np.random.normal(size = (m, )) * 0.01\n",
    "    \n",
    "    b = np.zeros((1, ))\n",
    "    \n",
    "    return W, b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 前向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们要完成输入矩阵$X$在神经网络中的计算，也就是完成 $Z = XW + b$ 的计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, b):\n",
    "    '''\n",
    "    前向传播，计算Z = XW + b\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 完成Z = XW + b的计算\n",
    "    Z = X.dot(W)+b                    # YOUR CODE HERE\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-28.37377228144393\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = forward(trainX, Wt, bt)\n",
    "print(tmp.mean()) # -28.37377"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来编写损失函数，我们以均方误差(MSE)作为损失函数，需要大家实现MSE的计算：\n",
    "\n",
    "$$\n",
    "\\mathrm{loss}(y, Z) = \\frac{1}{n} \\sum^n_{i = 1} (y_i - Z_i)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    '''\n",
    "    MSE，均方误差\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    y_pred: np.ndarray, shape = (n, )，预测值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    loss: float，损失值\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # 计算MSE\n",
    "    loss =  np.sum(np.power(y_true-y_pred,2))*1/len(y_true)                # YOUR CODE HERE\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39381033680.460075\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "tmp = mse(trainY, forward(trainX, Wt, bt))\n",
    "print(tmp) # 39381033680.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们要完成梯度的计算，也就是计算出损失函数对参数的偏导数的导数值：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial W} = \\frac{2}{n} X^\\mathrm{T} (Z - y)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathrm{loss}}{\\partial b} = \\frac{2}{n} \\sum^n_{i=1}(Z_i - y_i)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, Z, y_true):\n",
    "    '''\n",
    "    计算梯度\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    n = len(y_true)\n",
    "    \n",
    "    # 计算W的梯度\n",
    "    dW = X.T.dot((Z-y_true))*(2.0/n)            # YOUR CODE HERE\n",
    "    \n",
    "    # 计算b的梯度\n",
    "    db = np.sum(Z-y_true)*(2.0/n)             # YOUR CODE HERE\n",
    "    \n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3,)\n",
      "-1532030241.2528896\n",
      "-364308.5557637409\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print(dWt.shape) # (3,)\n",
    "print(dWt.mean()) # -1532030241.25\n",
    "print(dbt.mean()) # -182154.277882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 梯度下降"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这部分需要实现梯度下降的函数\n",
    "$$\n",
    "W := W - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial W}\n",
    "$$\n",
    "\n",
    "$$\n",
    "b := b - \\alpha \\frac{\\partial \\mathrm{loss}}{\\partial b}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(dW, db, W, b, learning_rate):\n",
    "    '''\n",
    "    梯度下降，参数更新，不需要返回值，W和b实际上是以引用的形式传入到函数内部，\n",
    "    函数内改变W和b会直接影响到它们本身，所以不需要返回值\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dW, np.ndarray, shape = (m, ), 参数W的梯度\n",
    "    \n",
    "    db, np.ndarray, shape = (1, ), 参数b的梯度\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 更新W\n",
    "    W -= learning_rate * dW\n",
    "    \n",
    "    # 更新b\n",
    "    b -= learning_rate * db\n",
    "    \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "(3,)\n",
      "15320302.416581338\n",
      "3643.0855576374092\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "Wt, bt = update(dWt, dbt, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 1821.54277882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完成整个参数更新的过程，先计算梯度，再更新参数，将compute_gradient和update组装在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(X, Z, y_true, W, b, learning_rate):\n",
    "    '''\n",
    "    使用compute_gradient和update函数，先计算梯度，再更新参数\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray, shape = (n, m)，输入的数据\n",
    "    \n",
    "    Z: np.ndarray, shape = (n, )，线性组合后的值\n",
    "    \n",
    "    y_true: np.ndarray, shape = (n, )，真值\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，权重\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，偏置\n",
    "    \n",
    "    learning_rate, float，学习率\n",
    "    \n",
    "    '''\n",
    "    # 计算参数的梯度\n",
    "    dW, db = compute_gradient(X, Z, y_true)                    # YOUR CODE HERE\n",
    "    \n",
    "    # 更新参数\n",
    "    # YOUR CODE HERE\n",
    "    W, b = update(dW,db,W,b,learning_rate)\n",
    "    \n",
    "    return W,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "(3,)\n",
      "15320302.416581338\n",
      "3643.0855576374092\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean()) # 0.00405243937693\n",
    "print(bt.mean()) # 0.0\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "Wt,bt = backward(trainX, Zt, trainY, Wt, bt, 0.01)\n",
    "\n",
    "print(Wt.shape) # (3,)\n",
    "print(Wt.mean()) # 15320302.4166\n",
    "print(bt.mean()) # 1821.54277882"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(trainX, trainY, testX, testY, W, b, epochs, learning_rate = 0.01, verbose = False):\n",
    "    '''\n",
    "    训练，我们要迭代epochs次，每次迭代的过程中，做一次前向传播和一次反向传播，更新参数\n",
    "    同时记录训练集和测试集上的损失值，后面画图用。然后循环往复，直到达到最大迭代次数epochs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    trainX: np.ndarray, shape = (n, m), 训练集\n",
    "    \n",
    "    trainY: np.ndarray, shape = (n, ), 训练集标记\n",
    "    \n",
    "    testX: np.ndarray, shape = (n_test, m)，测试集\n",
    "    \n",
    "    testY: np.ndarray, shape = (n_test, )，测试集的标记\n",
    "    \n",
    "    W: np.ndarray, shape = (m, )，参数W\n",
    "    \n",
    "    b: np.ndarray, shape = (1, )，参数b\n",
    "    \n",
    "    epochs: int, 要迭代的轮数\n",
    "    \n",
    "    learning_rate: float, default 0.01，学习率\n",
    "    \n",
    "    verbose: boolean, default False，是否打印损失值\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    training_loss_list = []\n",
    "    testing_loss_list = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # 这里我们要将神经网络的输出值保存起来，因为后面反向传播的时候需要这个值\n",
    "        Z = forward(trainX, W, b)\n",
    "        \n",
    "        # 计算训练集的损失值\n",
    "        training_loss = mse(trainY, Z)\n",
    "        \n",
    "        # 计算测试集的损失值        \n",
    "        testing_loss = mse(testY, forward(testX, W, b))\n",
    "        \n",
    "        # 将损失值存起来\n",
    "        training_loss_list.append(training_loss)\n",
    "        testing_loss_list.append(testing_loss)\n",
    "        \n",
    "        # 打印损失值，debug用\n",
    "        if verbose:\n",
    "            print('epoch %s training loss: %s'%(epoch+1, training_loss))\n",
    "            print('epoch %s testing loss: %s'%(epoch+1, testing_loss))\n",
    "            print()\n",
    "        \n",
    "        # 反向传播，参数更新\n",
    "        W, b = backward(trainX, Z, trainY, W, b, learning_rate)\n",
    "        \n",
    "    return training_loss_list, testing_loss_list, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.004052439376931716\n",
      "0.0\n",
      "[39381033680.460075, 3.390230782482131e+23]\n",
      "[38555252685.09387, 4.1516070231815876e+23]\n",
      "-57055790600890.93\n",
      "-8824267814.591057\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print(Wt.mean())          # 0.00405243937693\n",
    "print(bt.mean())          # 0.0\n",
    "\n",
    "training_loss_list, testing_loss_list, Wt, bt = train(trainX, trainY, testX, testY, Wt, bt, 2, learning_rate = 0.01, verbose = False)\n",
    "\n",
    "print(training_loss_list) # [39381033680.460075, 3.3902307664083424e+23]\n",
    "print(testing_loss_list)  # [38555252685.093872, 4.1516070070405267e+23]\n",
    "print(Wt.mean())          # -5.70557904608e+13\n",
    "print(bt.mean())          # -4412133889.08"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 检查\n",
    "\n",
    "编写一个绘制损失值变化曲线的函数\n",
    "\n",
    "一般我们通过绘制损失函数的变化曲线来判断模型的拟合状态。\n",
    "\n",
    "一般来说，随着迭代轮数的增加，训练集的loss在下降，而测试集的loss在上升，这说明我们正在不断地让模型在训练集上表现得越来越好，在测试集上表现得越来越糟糕，这就是过拟合的体现。  \n",
    "\n",
    "如果训练集loss和测试集loss共同下降，这就是我们想要的结果，说明模型正在很好的学习。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curve(training_loss_list, testing_loss_list):\n",
    "    '''\n",
    "    绘制损失值变化曲线\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    training_loss_list: list(float)，每迭代一次之后，训练集上的损失值\n",
    "    \n",
    "    testing_loss_list: list(float)，每迭代一次之后，测试集上的损失值\n",
    "    \n",
    "    '''\n",
    "    plt.figure(figsize = (10, 6))\n",
    "    plt.plot(training_loss_list, label = 'training loss')\n",
    "    plt.plot(testing_loss_list, label = 'testing loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面这些函数就是完成整个神经网络需要的函数了\n",
    "\n",
    "|函数名|功能|\n",
    "|-|-|\n",
    "|initialize | 参数初始化|\n",
    "|forward | 给定数据，计算神经网络的输出值|\n",
    "|mse | 给定真值，计算神经网络的预测值与真值之间的差距|\n",
    "|backward | 计算参数的梯度，并实现参数的更新|\n",
    "|compute_gradient | 计算参数的梯度|\n",
    "|update | 参数的更新|\n",
    "|backward | 计算参数梯度，并且更新参数|\n",
    "|train | 训练神经网络|\n",
    "|plot_loss_curve | 绘制损失函数的变化曲线|\n",
    "\n",
    "我们使用参数初始化函数和训练函数，完成神经网络的训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 training loss: 39381033680.460075\n",
      "epoch 1 testing loss: 38555252685.09387\n",
      "\n",
      "epoch 2 training loss: 3.390230782482131e+23\n",
      "epoch 2 testing loss: 4.1516070231815876e+23\n",
      "\n",
      "epoch 3 training loss: 5.0555031043473305e+36\n",
      "epoch 3 testing loss: 6.193449607112207e+36\n",
      "\n",
      "epoch 4 training loss: 7.538764120634838e+49\n",
      "epoch 4 testing loss: 9.235676243151287e+49\n",
      "\n",
      "epoch 5 training loss: 1.1241801912459146e+63\n",
      "epoch 5 testing loss: 1.3772236581164414e+63\n",
      "\n",
      "epoch 6 training loss: 1.6763770323182332e+76\n",
      "epoch 6 testing loss: 2.0537153445830344e+76\n",
      "\n",
      "epoch 7 training loss: 2.4998127314176716e+89\n",
      "epoch 7 testing loss: 3.06249946528851e+89\n",
      "\n",
      "epoch 8 training loss: 3.7277197024801475e+102\n",
      "epoch 8 testing loss: 4.566797925345661e+102\n",
      "\n",
      "epoch 9 training loss: 5.558774065598973e+115\n",
      "epoch 9 testing loss: 6.810007161577314e+115\n",
      "\n",
      "epoch 10 training loss: 8.28924156819442e+128\n",
      "epoch 10 testing loss: 1.0155079839059038e+129\n",
      "\n",
      "epoch 11 training loss: 1.2360913569254475e+142\n",
      "epoch 11 testing loss: 1.514325082057293e+142\n",
      "\n",
      "epoch 12 training loss: 1.843258915904184e+155\n",
      "epoch 12 testing loss: 2.2581609307764068e+155\n",
      "\n",
      "epoch 13 training loss: 2.7486669266187508e+168\n",
      "epoch 13 testing loss: 3.3673686381508715e+168\n",
      "\n",
      "epoch 14 training loss: 4.0988109745730865e+181\n",
      "epoch 14 testing loss: 5.0214187087646476e+181\n",
      "\n",
      "epoch 15 training loss: 6.112145215770998e+194\n",
      "epoch 15 testing loss: 7.487937484200652e+194\n",
      "\n",
      "epoch 16 training loss: 9.114428396533559e+207\n",
      "epoch 16 testing loss: 1.1166009253407036e+208\n",
      "\n",
      "epoch 17 training loss: 1.359143182350884e+221\n",
      "epoch 17 testing loss: 1.6650748341615076e+221\n",
      "\n",
      "epoch 18 training loss: 2.0267537466567308e+234\n",
      "epoch 18 testing loss: 2.4829588982402296e+234\n",
      "\n",
      "epoch 19 training loss: 3.0222943417058007e+247\n",
      "epoch 19 testing loss: 3.702587273475262e+247\n",
      "\n",
      "epoch 20 training loss: 4.506844061827689e+260\n",
      "epoch 20 testing loss: 5.521296597949002e+260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 特征数m\n",
    "m = trainX.shape[1]\n",
    "\n",
    "# 参数初始化\n",
    "W, b = initialize(m)\n",
    "\n",
    "# 训练20轮，学习率为0.01\n",
    "training_loss_list, testing_loss_list, W, b = train(trainX, trainY, testX, testY, W, b, 20, learning_rate = 0.01, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "绘制损失值的变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过打印损失的信息我们可以看到损失值持续上升，这就说明哪里出了问题。但是如果所有的测试样例都通过了，就说明我们的实现是没有问题的。运行下面的测试样例，观察哪里出了问题。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, W: [-0.00348894  0.00983703  0.00580923]\n",
      "epoch 0, b: [0.]\n",
      "\n",
      "dWt: LotArea      -4.181729e+09\n",
      "BsmtUnfSF    -2.198803e+08\n",
      "GarageArea   -1.944810e+08\n",
      "dtype: float64\n",
      "db: -364308.5557637409\n",
      "\n",
      "epoch 1, W: [-0.00348894  0.00983703  0.00580923]\n",
      "epoch 1, b: [3643.08555764]\n"
     ]
    }
   ],
   "source": [
    "# 测试样例\n",
    "Wt, bt = initialize(trainX.shape[1])\n",
    "print('epoch 0, W:', Wt)  # [-0.00348894  0.00983703  0.00580923]\n",
    "print('epoch 0, b:', bt)  # [ 0.]\n",
    "print()\n",
    "\n",
    "Zt = forward(trainX, Wt, bt)\n",
    "dWt, dbt = compute_gradient(trainX, Zt, trainY)\n",
    "print('dWt:', dWt) # [ -4.18172940e+09  -2.19880296e+08  -1.94481031e+08]\n",
    "print('db:', dbt) # -182154.277882\n",
    "print()\n",
    "\n",
    "update(dWt, dbt, Wt, bt, 0.01)\n",
    "print('epoch 1, W:', Wt)  # [ 41817293.96016914   2198802.97412493   1944810.31544994]\n",
    "print('epoch 1, b:', bt)  # [ 1821.54277882]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，我们最开始的参数都是在 $10^{-3}$ 这个数量级上，而第一轮迭代时计算出的梯度的数量级在 $10^8$ 左右，这就导致使用梯度下降更新的时候，让参数变成了 $10^6$ 这个数量级左右（学习率为0.01）。产生这样的问题的主要原因是：我们的原始数据 $X$ 没有经过适当的处理，直接扔到了神经网络中进行训练，导致在计算梯度时，由于 $X$ 的数量级过大，导致梯度的数量级变大，在参数更新时使得参数的数量级不断上升，导致参数无法收敛。\n",
    "\n",
    "解决的方法也很简单，对参数进行归一化处理，将其标准化，使均值为0，缩放到 $[-1, 1]$附近。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 标准化处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "标准化处理和第一题一样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tht/.local/lib/python3.6/site-packages/sklearn/preprocessing/data.py:617: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/home/tht/.local/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/home/tht/.local/lib/python3.6/site-packages/ipykernel_launcher.py:4: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler.\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "stand = StandardScaler()\n",
    "trainX_normalized = stand.fit_transform(trainX)\n",
    "testX_normalized = stand.transform(testX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新训练模型，这次我们迭代40轮，学习率设置为0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = trainX.shape[1]\n",
    "W, b = initialize(m)\n",
    "training_loss_list, testing_loss_list, W, b = train(trainX_normalized, trainY, testX_normalized, testY, W, b, 40, learning_rate = 0.1, verbose = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "打印损失值变化曲线"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_loss_curve(training_loss_list, testing_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算测试集上的MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60305.852679101554"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = forward(testX_normalized, W, b)\n",
    "mse(testY, prediction) ** 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
